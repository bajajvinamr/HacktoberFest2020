{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.014067,
     "end_time": "2020-09-19T01:26:41.220851",
     "exception": false,
     "start_time": "2020-09-19T01:26:41.206784",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Importing Libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.01277,
     "end_time": "2020-09-19T01:26:41.246927",
     "exception": false,
     "start_time": "2020-09-19T01:26:41.234157",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "In this notebook I'd like to continue on the work of [Atish Adhikari](https://www.kaggle.com/atishadhikari). In his [notebook](https://www.kaggle.com/atishadhikari/fake-news-cleaning-word2vec-lstm-99-accuracy), he proposes a novel approach for News Classification.\n",
    "\n",
    "We'll use the following modules, \n",
    "* [numpy](https://numpy.org/doc/stable/reference/index.html)\n",
    "* [pandas](https://pandas.pydata.org/docs/reference/index.html)\n",
    "* [tensorflow](https://www.tensorflow.org/api_docs/python/tf)\n",
    "* [tensorflow_datasets](https://www.tensorflow.org/datasets/overview?hl=en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2020-09-19T01:26:41.279889Z",
     "iopub.status.busy": "2020-09-19T01:26:41.279123Z",
     "iopub.status.idle": "2020-09-19T01:26:47.100049Z",
     "shell.execute_reply": "2020-09-19T01:26:47.101168Z"
    },
    "papermill": {
     "duration": 5.841509,
     "end_time": "2020-09-19T01:26:47.101531",
     "exception": false,
     "start_time": "2020-09-19T01:26:41.260022",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/fake-and-real-news-dataset/True.csv\n",
      "/kaggle/input/fake-and-real-news-dataset/Fake.csv\n"
     ]
    }
   ],
   "source": [
    "import numpy as np # For Linear Algebra\n",
    "import pandas as pd # For I/O, Data Transformation\n",
    "import tensorflow as tf # Tensorflow\n",
    "import tensorflow_datasets as tfds # For the SubTextEncoder\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.057071,
     "end_time": "2020-09-19T01:26:47.177509",
     "exception": false,
     "start_time": "2020-09-19T01:26:47.120438",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Pre-Processing and Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.015404,
     "end_time": "2020-09-19T01:26:47.207224",
     "exception": false,
     "start_time": "2020-09-19T01:26:47.191820",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "The original dataset doesn't have any class variables associated with the instances. Thus, to enable supervised learning we add another \"**class**\" variable to the DataFrames. Also, to get a reliable and authentic score for classification we concatenate the \"**text**\" and \"**title**\" columns. We then drop the redundant columns from both the DataFrames. Then, we just make a single DataFrame out of both the DataFrames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "execution": {
     "iopub.execute_input": "2020-09-19T01:26:47.252752Z",
     "iopub.status.busy": "2020-09-19T01:26:47.251875Z",
     "iopub.status.idle": "2020-09-19T01:26:48.525690Z",
     "shell.execute_reply": "2020-09-19T01:26:48.525090Z"
    },
    "papermill": {
     "duration": 1.304776,
     "end_time": "2020-09-19T01:26:48.525804",
     "exception": false,
     "start_time": "2020-09-19T01:26:47.221028",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "fakedataset = pd.read_csv(\"/kaggle/input/fake-and-real-news-dataset/Fake.csv\") # Make a DataFrame for Fake News\n",
    "realdataset = pd.read_csv(\"/kaggle/input/fake-and-real-news-dataset/True.csv\") # Make a DataFrame for Real News\n",
    "realdataset[\"class\"] = 1 # Adding Class to Real News\n",
    "fakedataset[\"class\"] = 0 # Adding Class to Fake News\n",
    "realdataset[\"text\"] = realdataset[\"title\"] + \" \" + realdataset[\"text\"] # Concatenating Text and Title into a single column for Real News DataFrame\n",
    "fakedataset[\"text\"] = fakedataset[\"title\"] + \" \" + fakedataset[\"text\"] # Concatenating Text and Title into a single column for Fake News DataFrame\n",
    "realdataset.drop([\"subject\", \"date\", \"title\"], axis = 1) # Removing Redundant features from Real News DataFrame\n",
    "fakedataset.drop([\"subject\", \"date\", \"title\"], axis = 1) # Removing Redundant features from Fake News DataFrame\n",
    "dataset = realdataset.append(fakedataset, ignore_index = True) # Making a Single DataFrame \n",
    "del realdataset, fakedataset "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.013641,
     "end_time": "2020-09-19T01:26:48.553435",
     "exception": false,
     "start_time": "2020-09-19T01:26:48.539794",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Encoding the Corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.013641,
     "end_time": "2020-09-19T01:26:48.581323",
     "exception": false,
     "start_time": "2020-09-19T01:26:48.567682",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "To encode the corpus, we use the [**SubwordTextEncoder**](https://www.tensorflow.org/datasets/api_docs/python/tfds/features/text/SubwordTextEncoder) from tfds.features.text's **build_from_corpus** function. We declare a novel vocab_size of 10,000 and then use the \"**text**\" column from the DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-19T01:26:48.654603Z",
     "iopub.status.busy": "2020-09-19T01:26:48.633567Z",
     "iopub.status.idle": "2020-09-19T01:36:43.560579Z",
     "shell.execute_reply": "2020-09-19T01:36:43.560031Z"
    },
    "papermill": {
     "duration": 594.965715,
     "end_time": "2020-09-19T01:36:43.560702",
     "exception": false,
     "start_time": "2020-09-19T01:26:48.594987",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "vocab_size = 10000\n",
    "encoder = tfds.features.text.SubwordTextEncoder.build_from_corpus(dataset[\"text\"], vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.013713,
     "end_time": "2020-09-19T01:36:43.588632",
     "exception": false,
     "start_time": "2020-09-19T01:36:43.574919",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Here, we create a function to encode the DataFrame by looping through all the sentences in the corpus, with \"**post**\" padding using the [**tf.keras.preprocessing.sequence.pad_sequences()**](https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/sequence/pad_sequences?hl=en) function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-19T01:36:43.642612Z",
     "iopub.status.busy": "2020-09-19T01:36:43.629160Z",
     "iopub.status.idle": "2020-09-19T01:38:05.009691Z",
     "shell.execute_reply": "2020-09-19T01:38:05.009029Z"
    },
    "papermill": {
     "duration": 81.407403,
     "end_time": "2020-09-19T01:38:05.009816",
     "exception": false,
     "start_time": "2020-09-19T01:36:43.602413",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def enc(dataframe):\n",
    "    tokenized = []\n",
    "    for sentence in dataframe[\"text\"].values:\n",
    "        tokenized.append(encoder.encode(sentence))\n",
    "    out = tf.keras.preprocessing.sequence.pad_sequences(tokenized, padding = \"post\")\n",
    "    return out\n",
    "x = enc(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.013967,
     "end_time": "2020-09-19T01:38:05.038617",
     "exception": false,
     "start_time": "2020-09-19T01:38:05.024650",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Using the \"**class**\" column of the Dataset for Supervised Training of the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-19T01:38:05.073087Z",
     "iopub.status.busy": "2020-09-19T01:38:05.072346Z",
     "iopub.status.idle": "2020-09-19T01:38:05.078402Z",
     "shell.execute_reply": "2020-09-19T01:38:05.079085Z"
    },
    "papermill": {
     "duration": 0.0265,
     "end_time": "2020-09-19T01:38:05.079211",
     "exception": false,
     "start_time": "2020-09-19T01:38:05.052711",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0        1\n",
      "1        1\n",
      "2        1\n",
      "3        1\n",
      "4        1\n",
      "        ..\n",
      "44893    0\n",
      "44894    0\n",
      "44895    0\n",
      "44896    0\n",
      "44897    0\n",
      "Name: class, Length: 44898, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "y = dataset[\"class\"]\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.014401,
     "end_time": "2020-09-19T01:38:05.108129",
     "exception": false,
     "start_time": "2020-09-19T01:38:05.093728",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Model Definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.014294,
     "end_time": "2020-09-19T01:38:05.137088",
     "exception": false,
     "start_time": "2020-09-19T01:38:05.122794",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Here, we define our Model with the following layers:\n",
    "\n",
    "* [Embedding Layer](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Embedding?hl=en)\n",
    "* [Bidirectional LSTM Layer](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Bidirectional?hl=en) with 64 units\n",
    "* [Bidirectional LSTM Layer](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Bidirectional?hl=en) with 32 units\n",
    "* [Dense Layer](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dense?hl=en) with 64 units\n",
    "* [Dropout Layer](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dropout?hl=en) with a 50% droprate\n",
    "* [Dense](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dense?hl=en) with a single output unit\n",
    "\n",
    "We then compile the model using:\n",
    "\n",
    "* [Adam Optimiser](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/Adam?hl=en)\n",
    "* [Binary Crossentropy Loss](https://www.tensorflow.org/api_docs/python/tf/keras/losses/BinaryCrossentropy?hl=en)\n",
    "* Metrics as [Accuracy](https://www.tensorflow.org/api_docs/python/tf/keras/metrics/Accuracy?hl=en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-19T01:38:05.176243Z",
     "iopub.status.busy": "2020-09-19T01:38:05.175627Z",
     "iopub.status.idle": "2020-09-19T01:38:08.833552Z",
     "shell.execute_reply": "2020-09-19T01:38:08.834339Z"
    },
    "papermill": {
     "duration": 3.682791,
     "end_time": "2020-09-19T01:38:08.834546",
     "exception": false,
     "start_time": "2020-09-19T01:38:05.151755",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Model Definition\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(encoder.vocab_size, 64), # Embedding Layer using the vocab-size from encoder\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64,  return_sequences=True)), # Create the first Bidirectional layer with 64 LSTM units\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32)), # Second Bidirectional layer witth 32 LSTM units\n",
    "    tf.keras.layers.Dense(64, activation='relu'), # A Dense Layer with 64 units\n",
    "    tf.keras.layers.Dropout(0.5), # 50% Dropout\n",
    "    tf.keras.layers.Dense(1) # Final Dense layer with a single unit\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics= ['acc']) # Compiling the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.020672,
     "end_time": "2020-09-19T01:38:08.876959",
     "exception": false,
     "start_time": "2020-09-19T01:38:08.856287",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Training the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.02071,
     "end_time": "2020-09-19T01:38:08.918383",
     "exception": false,
     "start_time": "2020-09-19T01:38:08.897673",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "We train the model for a novel 2 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-19T01:38:08.966534Z",
     "iopub.status.busy": "2020-09-19T01:38:08.965917Z",
     "iopub.status.idle": "2020-09-19T03:01:54.637253Z",
     "shell.execute_reply": "2020-09-19T03:01:54.636337Z"
    },
    "papermill": {
     "duration": 5025.697485,
     "end_time": "2020-09-19T03:01:54.637395",
     "exception": false,
     "start_time": "2020-09-19T01:38:08.939910",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "1404/1404 [==============================] - 2491s 2s/step - loss: 0.0611 - acc: 0.9806\n",
      "Epoch 2/2\n",
      "1404/1404 [==============================] - 2521s 2s/step - loss: 0.0043 - acc: 0.9993\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(x,y, epochs = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.9772,
     "end_time": "2020-09-19T03:01:56.799193",
     "exception": false,
     "start_time": "2020-09-19T03:01:55.821993",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Predicting with the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.953457,
     "end_time": "2020-09-19T03:01:58.710773",
     "exception": false,
     "start_time": "2020-09-19T03:01:57.757316",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Here, we write 2 functions to predict using the model. A pad_to_size function to pad our vectors and a sample_predict function to encode a string and predict using the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-19T03:02:00.624044Z",
     "iopub.status.busy": "2020-09-19T03:02:00.623104Z",
     "iopub.status.idle": "2020-09-19T03:02:01.870965Z",
     "shell.execute_reply": "2020-09-19T03:02:01.871659Z"
    },
    "papermill": {
     "duration": 2.207705,
     "end_time": "2020-09-19T03:02:01.871881",
     "exception": false,
     "start_time": "2020-09-19T03:01:59.664176",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.25145346]]\n"
     ]
    }
   ],
   "source": [
    "def pad_to_size(vec, size):\n",
    "  zero = [0] * (size - len(vec))\n",
    "  vec.extend(zeros)\n",
    "  return vec\n",
    "\n",
    "def sample_predict(sample_pred_text, pad):\n",
    "  encoded_sample_pred_text = encoder.encode(sample_pred_text)\n",
    "\n",
    "  if pad:\n",
    "    encoded_sample_pred_text = pad_to_size(encoded_sample_pred_text, 64)\n",
    "  encoded_sample_pred_text = tf.cast(encoded_sample_pred_text, tf.float32)\n",
    "  predictions = model.predict(tf.expand_dims(encoded_sample_pred_text, 0))\n",
    "\n",
    "  return (predictions)\n",
    "\n",
    "sample_pred_text = ('The movie was cool. The animation and the graphics')\n",
    "predictions = sample_predict(sample_pred_text, pad=False)\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.99379,
     "end_time": "2020-09-19T03:02:03.826610",
     "exception": false,
     "start_time": "2020-09-19T03:02:02.832820",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    " # Download the Model Weights for Yourself"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-19T03:02:05.783670Z",
     "iopub.status.busy": "2020-09-19T03:02:05.782776Z",
     "iopub.status.idle": "2020-09-19T03:02:05.848123Z",
     "shell.execute_reply": "2020-09-19T03:02:05.848597Z"
    },
    "papermill": {
     "duration": 1.040783,
     "end_time": "2020-09-19T03:02:05.848742",
     "exception": false,
     "start_time": "2020-09-19T03:02:04.807959",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<a href='my_model.h5' target='_blank'>my_model.h5</a><br>"
      ],
      "text/plain": [
       "/kaggle/working/my_model.h5"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.save('my_model.h5') \n",
    "import os\n",
    "from IPython.display import FileLink\n",
    "FileLink(r'my_model.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "papermill": {
   "duration": 5731.83436,
   "end_time": "2020-09-19T03:02:09.000427",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2020-09-19T01:26:37.166067",
   "version": "2.1.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

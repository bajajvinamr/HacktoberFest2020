# -*- coding: utf-8 -*-
"""Untitled6.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Znp0qHd7kksuCy9Nsrd2lI3hUH-xxvFN
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

pwd

train = pd.read_csv("train.csv")
train

train.head()

train = train.dropna()
x = train['x'].values
y = train['y'].values

X = train['x'].values
Y = train['y'].values
# mean of our inputs and outputs
x_mean = np.mean(X)
y_mean = np.mean(Y)
#total number of values
n = len(X)
# using the formula to calculate the b1 and b0
numerator = 0
denominator = 0
for i in range(n):
    numerator += (X[i] - x_mean) * (Y[i] - y_mean)
    denominator += (X[i] - x_mean) ** 2

b1 = numerator / denominator
b0 = y_mean - (b1 * x_mean)
#printing the coefficient
print(b1, b0)

#plotting values
x_max = np.max(X) + 100
x_min = np.min(X) - 100
#calculating line values of x and y
x = np.linspace(x_min, x_max, 1000)
y = b0 + b1 * x
#plotting line
plt.plot(x, y, color='#00ff00', label='Linear Regression')
#plot the data point
plt.scatter(X, Y, color='#ff0000', label='Data Point')
# x-axis label
plt.xlabel('x')
#y-axis label
plt.ylabel('y')
plt.legend()
plt.show()

rmse = 0
for i in range(n):
    y_pred=  b0 + b1* X[i]
    rmse += (Y[i] - y_pred) ** 2

rmse = np.sqrt(rmse/n)
print(rmse)

sumofsquares = 0
sumofresiduals = 0
for i in range(n) :
    y_pred = b0 + b1 * X[i]
    sumofsquares += (Y[i] - y_mean) ** 2
    sumofresiduals += (Y[i] - y_pred) **2

score  = 1 - (sumofresiduals/sumofsquares)
print(score)

def hypothesis(theta0, theta1, x):
	return theta0 + (theta1*x)

def plotLine(theta0, theta1, x, y):
	max_x = np.max(x) + 100
	min_x = np.min(x) - 100

	xplot = np.linspace(min_x, max_x, 1000)
	yplot = theta0 + theta1 * xplot

	plt.plot(xplot, yplot, color='#ff0000', label='Regression Line')

	plt.scatter(x,y)
	plt.axis([-10, 10, 0, 200])
	plt.show()

theta0 = np.random.rand()
theta1 = np.random.rand()

plotLine(theta0, theta1, x, y)

def cost(theta0, theta1, X, y):
	costValue = 0
	for (xi, yi) in zip(X, y):
		costValue += 0.5 * ((hypothesis(theta0, theta1, xi) - yi)**2)
	return costValue

def derivatives(theta0, theta1, x, y):
	dtheta0 = 0
	dtheta1 = 0
	for (xi, yi) in zip(x, y):
		dtheta0 += hypothesis(theta0, theta1, xi) - yi
		dtheta1 += (hypothesis(theta0, theta1, xi) - yi)*xi

	dtheta0 /= len(x)
	dtheta1 /= len(x)

	return dtheta0, dtheta1

def LinearRegression(x, y):
	theta0 = np.random.rand()
	theta1 = np.random.rand()

	for i in range(0, 1000):
		if i % 100 == 0:
			plotLine(theta0, theta1, x, y)
		# print(cost(theta0, theta1, x, y))
		theta0, theta1 = updateParameters(theta0, theta1, x, y, 0.005)

def plotLine(theta0, theta1, x, y):
	max_x = np.max(x) + 100
	min_x = np.min(x) - 100

	xplot = np.linspace(min_x, max_x, 1000)
	yplot = theta0 + theta1 * xplot

	plt.plot(xplot, yplot, color='#ff0000', label='Regression Line')

	plt.scatter(x,y)
	plt.axis([-10, 10, 0, 200])
	plt.show()

theta0 = np.random.rand()
theta1 = np.random.rand()

plotLine(theta0, theta1, x, y)
